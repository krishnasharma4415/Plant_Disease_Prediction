{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (299, 299)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(299, scale=(0.8, 1.0)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"Cotton_Disease\"\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "\n",
    "train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = ImageFolder(val_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True)\n",
    "inception = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
    "inception.aux_logits = False \n",
    "resnet = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [vgg, inception, resnet]:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_fc = vgg.classifier[0].in_features\n",
    "vgg.classifier = nn.Sequential(nn.Linear(vgg_fc, 512), nn.ReLU(), nn.Linear(512, num_classes))\n",
    "\n",
    "inception_fc = inception.fc.in_features\n",
    "inception.fc = nn.Sequential(nn.Linear(inception_fc, 512), nn.ReLU(), nn.Linear(512, num_classes))\n",
    "\n",
    "resnet_fc = resnet.fc.in_features\n",
    "resnet.fc = nn.Sequential(nn.Linear(resnet_fc, 512), nn.ReLU(), nn.Linear(512, num_classes))\n",
    "\n",
    "vgg, inception, resnet = vgg.to(device), inception.to(device), resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, vgg, inception, resnet, num_classes):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.vgg = vgg\n",
    "        self.inception = inception\n",
    "        self.resnet = resnet\n",
    "        self.fc = nn.Linear(3 * num_classes, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vgg_out = self.vgg(x)\n",
    "        inception_out = self.inception(x)\n",
    "        resnet_out = self.resnet(x)\n",
    "\n",
    "        combined = torch.cat((vgg_out, inception_out, resnet_out), dim=1)\n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnsembleModel(vgg, inception, resnet, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\"Validation Accuracy: {val_acc:.2f}%\\n\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model, \"best_model.pth\")\n",
    "            print(\"✅ Best Model Saved!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 39.3204, Train Accuracy: 76.47%\n",
      "Validation Accuracy: 86.17%\n",
      "\n",
      "✅ Best Model Saved!\n",
      "\n",
      "Epoch [2/20], Loss: 11.7619, Train Accuracy: 93.59%\n",
      "Validation Accuracy: 93.68%\n",
      "\n",
      "✅ Best Model Saved!\n",
      "\n",
      "Epoch [3/20], Loss: 7.9508, Train Accuracy: 95.54%\n",
      "Validation Accuracy: 87.75%\n",
      "\n",
      "Epoch [4/20], Loss: 8.1321, Train Accuracy: 95.44%\n",
      "Validation Accuracy: 93.68%\n",
      "\n",
      "Epoch [5/20], Loss: 5.6873, Train Accuracy: 96.62%\n",
      "Validation Accuracy: 95.65%\n",
      "\n",
      "✅ Best Model Saved!\n",
      "\n",
      "Epoch [6/20], Loss: 5.5982, Train Accuracy: 96.67%\n",
      "Validation Accuracy: 94.07%\n",
      "\n",
      "Epoch [7/20], Loss: 4.9886, Train Accuracy: 97.13%\n",
      "Validation Accuracy: 97.63%\n",
      "\n",
      "✅ Best Model Saved!\n",
      "\n",
      "Epoch [8/20], Loss: 3.4788, Train Accuracy: 98.15%\n",
      "Validation Accuracy: 97.63%\n",
      "\n",
      "Epoch [9/20], Loss: 4.8692, Train Accuracy: 97.13%\n",
      "Validation Accuracy: 90.91%\n",
      "\n",
      "Epoch [10/20], Loss: 3.1719, Train Accuracy: 98.36%\n",
      "Validation Accuracy: 94.07%\n",
      "\n",
      "Epoch [11/20], Loss: 2.5960, Train Accuracy: 98.46%\n",
      "Validation Accuracy: 95.65%\n",
      "\n",
      "Epoch [12/20], Loss: 2.0740, Train Accuracy: 98.77%\n",
      "Validation Accuracy: 96.05%\n",
      "\n",
      "Epoch [13/20], Loss: 3.0287, Train Accuracy: 98.26%\n",
      "Validation Accuracy: 95.26%\n",
      "\n",
      "Epoch [14/20], Loss: 2.1352, Train Accuracy: 98.87%\n",
      "Validation Accuracy: 95.26%\n",
      "\n",
      "Epoch [15/20], Loss: 1.4761, Train Accuracy: 99.44%\n",
      "Validation Accuracy: 94.47%\n",
      "\n",
      "Epoch [16/20], Loss: 1.2528, Train Accuracy: 99.23%\n",
      "Validation Accuracy: 96.44%\n",
      "\n",
      "Epoch [17/20], Loss: 2.9226, Train Accuracy: 98.36%\n",
      "Validation Accuracy: 95.26%\n",
      "\n",
      "Epoch [18/20], Loss: 5.0299, Train Accuracy: 96.72%\n",
      "Validation Accuracy: 95.65%\n",
      "\n",
      "Epoch [19/20], Loss: 2.3107, Train Accuracy: 98.62%\n",
      "Validation Accuracy: 93.28%\n",
      "\n",
      "Epoch [20/20], Loss: 2.5620, Train Accuracy: 98.36%\n",
      "Validation Accuracy: 98.42%\n",
      "\n",
      "✅ Best Model Saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
